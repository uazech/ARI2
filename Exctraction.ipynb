{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette première cellule vous présente la manière avec laquelle j'ai obtenu les features/caractéristiques à partir des jeux de données de \"textes\"\n",
    "Pour obtenir les textes (et les \"filtrer\") j'ai utiliser le script ./textDatasets/conversion.sh (qui prends, par exemple les fichiers FR_* les concatene, supprime les accents (en gardant les caracteres sans accents) et enfin transforme en minuscules).\n",
    "J'ai utilisé les livres \"open data\" de la base du site Guttenberg Project (qui archive énormément de livres en texte, \"txt\" simples\", tombés dans le domaine publique)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import string\n",
    "BlockSz=1000\n",
    "\n",
    "# On procede de la même façon pour les differents languages\n",
    "# Ouverture du fichier texte contenant les caracteres alphabetiques \"brutes\"\n",
    "textGERMAN = open('./textDatasets/GERMAN')\n",
    "# Lecture\n",
    "DE=textGERMAN.read()\n",
    "textGERMAN.close()\n",
    "# Calcule du nombre de blocks (arrondi par defaut) de BlockSz caracteres\n",
    "NbBlock=int(np.floor(len(DE)/BlockSz))\n",
    "# Creation des tableaux de donnees textuelles et de features\n",
    "DataDE=[]\n",
    "FeaDE =[]\n",
    "# Pour tous les indices de blocks ...\n",
    "for index in range(NbBlock):\n",
    "    # Exctraction des donnees texte du block\n",
    "    DataDE.append(DE[index*BlockSz:(index+1)*BlockSz])\n",
    "    # Comptage de chacun des caracteres\n",
    "    countDE=[]\n",
    "    for letter in string.ascii_lowercase:\n",
    "        countDE.append(DataDE[index].count(letter))\n",
    "    # Comptage de toutes les 26**2 paires de caracteres possibles\n",
    "    for letter1 in string.ascii_lowercase:\n",
    "        for letter2 in string.ascii_lowercase:\n",
    "            countDE.append(DataDE[index].count(letter1+letter2))\n",
    "    # Une fois le vecteur de décompte obtenu  .... on ajoute ce vecteur (contenant tous les decomptes) au tableaux des features\n",
    "    FeaDE.append(countDE)\n",
    "\n",
    "# On procede de la même façon pour les differents languages ....\n",
    "textFRENCH = open('./textDatasets/FRENCH')\n",
    "FR=textFRENCH.read()\n",
    "textFRENCH.close()\n",
    "NbBlock=int(np.floor(len(FR)/BlockSz))\n",
    "DataFR=[]\n",
    "FeaFR =[]\n",
    "for index in range(NbBlock):\n",
    "    DataFR.append(FR[index*BlockSz:(index+1)*BlockSz])\n",
    "    countFR=[]\n",
    "    for letter in string.ascii_lowercase:\n",
    "        countFR.append(DataFR[index].count(letter))\n",
    "    for letter1 in string.ascii_lowercase:\n",
    "        for letter2 in string.ascii_lowercase:\n",
    "            countFR.append(DataFR[index].count(letter1+letter2))\n",
    "    FeaFR.append(countFR)\n",
    "\n",
    "# On procede de la même façon pour les differents languages ....\n",
    "textSPANISH = open('./textDatasets/SPANISH')\n",
    "ES=textSPANISH.read()\n",
    "textSPANISH.close()\n",
    "NbBlock=int(np.floor(len(ES)/BlockSz))\n",
    "DataES=[]\n",
    "FeaES =[]\n",
    "for index in range(NbBlock):\n",
    "    DataES.append(ES[index*BlockSz:(index+1)*BlockSz])\n",
    "\n",
    "    countES=[]\n",
    "    for letter in string.ascii_lowercase:\n",
    "        countES.append(DataES[index].count(letter))\n",
    "    for letter1 in string.ascii_lowercase:\n",
    "        for letter2 in string.ascii_lowercase:\n",
    "            countES.append(DataES[index].count(letter1+letter2))\n",
    "    FeaES.append(countES)\n",
    "\n",
    "# On procede de la même façon pour les differents languages ....\n",
    "textENGLISH = open('./textDatasets/ENGLISH')\n",
    "EN=textENGLISH.read()\n",
    "textENGLISH.close()\n",
    "NbBlock=int(np.floor(len(EN)/BlockSz))\n",
    "DataEN=[]\n",
    "FeaEN =[]\n",
    "for index in range(NbBlock):\n",
    "    DataEN.append(EN[index*BlockSz:(index+1)*BlockSz])\n",
    "    countEN=[]\n",
    "    for letter in string.ascii_lowercase:\n",
    "        countEN.append(DataEN[index].count(letter))\n",
    "    for letter1 in string.ascii_lowercase:\n",
    "        for letter2 in string.ascii_lowercase:\n",
    "            countEN.append(DataEN[index].count(letter1+letter2))\n",
    "    FeaEN.append(countEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ici on sauvegarde les caracteristiques, pour eviter de les recalculer à chaque fois ....\n",
    "np.save('./FeaEN.npy', FeaEN)\n",
    "np.save('./FeaFR', FeaFR)\n",
    "np.save('./FeaDE', FeaDE)\n",
    "np.save('./FeaES', FeaES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... On peut simplement les loader (et commenter les cellules precedentes)\n",
    "FeaEN = np.load('./FeaEN.npy')\n",
    "FeaFR = np.load('./FeaFR.npy')\n",
    "FeaDE = np.load('./FeaDE.npy')\n",
    "\n",
    "FeaES = np.load('./FeaES.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(4329, 702)\n(4116, 702)\n(4321, 702)\n(4209, 702)\n"
    }
   ],
   "source": [
    "# Pour plus de \"facilier de manipulation\" on trasforme les donnes en \"matrice numpy\" et on affiche la dimension de la base de donnees.\n",
    "FeaDE = np.array(FeaDE)\n",
    "print(FeaDE.shape)\n",
    "\n",
    "FeaFR = np.array(FeaFR)\n",
    "print(FeaFR.shape)\n",
    "\n",
    "FeaES = np.array(FeaES)\n",
    "print(FeaES.shape)\n",
    "\n",
    "FeaEN = np.array(FeaEN)\n",
    "print(FeaEN.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* 1) Réaliser une classification binaire en utilisant les méthodes linéaires suivantes: regression \"ridge\", LASSO et SVM (sans noyau !).\n",
    " * Pour ces méthodes vous devez faire une recherche du meilleur paramètre de régularisation ;\n",
    " * Vous devrez égalemement selectionner les deux languages de votre choix"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On choisit de différencier les langages DE et FR.\n",
    "On commence par définir nos matrices d'apprentissage et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "dfDE = pd.DataFrame(FeaDE)\n",
    "dfDE[\"lang\"]=0\n",
    "\n",
    "dfFR = pd.DataFrame(FeaFR)\n",
    "dfFR[\"lang\"]=1\n",
    "\n",
    "df = pd.concat([dfDE, dfFR])\n",
    "df.head()\n",
    "y = df.pop(\"lang\")\n",
    "X = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On réalise une classification des données en utilisant une régression logistique (et non linéaire, comme indiqué, qui correspond à un problème de régression)."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-9-b0d4597636be>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-b0d4597636be>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    On commence par rechercher les meilleurs paramètres de régularisation, à l'aide de la librairie GridSearchCV de sklearn.\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "On commence par rechercher les meilleurs paramètres de régularisation, à l'aide de la librairie GridSearchCV de sklearn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 0.2782559402207124, 'penalty': 'l2'}"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg=LogisticRegression(solver=\"liblinear\")\n",
    "grid={\n",
    "    \"C\": np.logspace(-1,1,10),\n",
    "    \"penalty\": [\n",
    "        \"l1\", # l1 : lasso\n",
    "        \"l2\" # l2 : ridge\n",
    "    ]\n",
    "}\n",
    "\n",
    "logreg_cv=GridSearchCV(logreg, grid, cv=10)\n",
    "logreg_cv.fit(X_train,y_train)\n",
    "logreg_cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On en conclut que la régularisation qui résoud le mieux ce problème est une régularisation L2 (ridge), qui enlève les features les moins importantes.\n",
    "\n",
    "Le coût optimal est de 0.27\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy on test : 0.9988158673771462\n"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression(solver=\"liblinear\", penalty=logreg_cv.best_params_[\"penalty\"], C=logreg_cv.best_params_['C'])\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred = logreg.predict(X_test)\n",
    "print(f\"Accuracy on test : {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "On réalise enfin une recherche des meilleurs paramètres à l'aide de GridSearchCV le SVM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:    2.9s\n[Parallel(n_jobs=-1)]: Done 200 out of 200 | elapsed:    9.6s finished\nC:\\Users\\Maitre\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n  \"the number of iterations.\", ConvergenceWarning)\n"
    },
    {
     "data": {
      "text/plain": "{'C': 2.1544346900318834, 'penalty': 'l1'}"
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "parameters ={'C':np.logspace(-10, 10, 5)}\n",
    "\n",
    "grid = {\n",
    "    'C': np.logspace(-1,1,10),\n",
    "    'penalty' : [\"l1\", \"l2\"]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(svm.LinearSVC(dual=False), grid, cv=10, verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_test, y_test, )\n",
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'C': 1, 'gamma': 0.001}"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "On obtient donc 2 meilleurs paramètres (penalty=l1 et C=2.15).\n",
    "\n",
    "On entraine tout le modèle avec ces 2 paramètres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy on test : 0.9994079336885732\n"
    }
   ],
   "source": [
    "SVM_model = svm.LinearSVC(dual=False, penalty=grid_search.best_params_[\"penalty\"], C=grid_search.best_params_[\"C\"], max_iter=10000)\n",
    "SVM_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = SVM_model.predict(X_test)\n",
    "print(f\"Accuracy on test : {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Le modèle de type SVM est globalement plus long à construire qu'un modèle de type régression logistique.\n",
    "Cependant, les résultats obtenus sont légérements meilleurs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* 2) Réaliser une classification binaire avec l'une des méthodes linéaire précédent en utilisant la réduction de dimension (ACP par exemple, ou une autre méthode)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On entraine et applique l'ACP sur le jeu de train. \n",
    "\n",
    "\n",
    "Par la suite, on applique l'ACP avec les dimensions précemment apprises sur le jeu de test, pour vérifier que les dimensions apprises sont bien généralisables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(6756, 30)"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_components=30\n",
    "pca = PCA(n_components=n_components)\n",
    "X_train_transform = pca.fit_transform(X_train)\n",
    "X_test_transform = pca.transform(X_test)\n",
    "X_train_transform.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Accuracy on test : 0.9988158673771462\n"
    }
   ],
   "source": [
    "logreg = LogisticRegression(solver=\"liblinear\", penalty=logreg_cv.best_params_[\"penalty\"], C=logreg_cv.best_params_['C'])\n",
    "\n",
    "logreg.fit(X_train_transform, y_train)\n",
    "y_pred = logreg.predict(X_test_transform)\n",
    "print(f\"Accuracy on test : {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Les résultats obtenus sont identiques à ceux obtenus précédemments. \n",
    "\n",
    "Cela veut dire que le jeu de données est de dimension trop importante et que ces dimensions peuvent être beaucoup réduites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* 4. Réaliser une classification binaire en utilisant les SVM à noyau (comparer les performances obtenus avec un noyau Gaussien (rbf) et un noyau polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'gamma': 'scale'}"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {\n",
    "    \"gamma\" : [\"auto\", \"scale\"]\n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(svm.SVC(kernel=\"rbf\"), grid, cv=10, verbose=1, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9964476021314387"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_kernel = svm.SVC(kernel=\"rbf\", gamma=gridSearch.best_params_[\"gamma\"])\n",
    "SVM_kernel.fit(X_train, y_train)\n",
    "y_pred=SVM_kernel.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-74-21beb3755b49>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-74-21beb3755b49>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    On obtient des résultats qui sont légèrements moins bons que ceux obtenus précédemments avec un noyau linéaire.\u001b[0m\n\u001b[1;37m             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "On obtient des résultats qui sont légèrements moins bons que ceux obtenus précédemments avec un noyau linéaire.\n",
    "\n",
    "On réalise la même opération avec un noyau polynomial\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Fitting 10 folds for each of 2 candidates, totalling 20 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    6.2s finished\n"
    },
    {
     "data": {
      "text/plain": "{'gamma': 'auto'}"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid = {\n",
    "    \"gamma\" : [\"auto\", \"scale\"]\n",
    "}\n",
    "\n",
    "gridSearch = GridSearchCV(svm.SVC(kernel=\"poly\"), grid, cv=10, verbose=1, n_jobs=-1)\n",
    "gridSearch.fit(X_train, y_train)\n",
    "gridSearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "0.9674363528715216"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SVM_kernel = svm.SVC(kernel=\"rbf\", gamma=gridSearch.best_params_[\"gamma\"])\n",
    "SVM_kernel.fit(X_train, y_train)\n",
    "y_pred=SVM_kernel.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Les résultats obtenus sont très en dessous des résultats précédents.\n",
    "\n",
    "Ce type de noyau ne sera donc pas retenu pour la suite du travail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* 5. Sur la base des résultats précédents, quelle est la méthode linéaire la plus adaptée à ce problème de classification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sur la base des résultats précédents et en utilisant comme métrique de décision l'accuracy, la méthode la mieux adaptée pour résoudre ce problème de classification est le SVM linéaire.\n",
    "Il s'agit de la seule méthode permettant d'avoir une accuracy supérieur à 99,9%.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions:\n",
    "* 6. Enfin, mettre en place une méthode (de votre choix) de classification multi-classe;\n",
    " * Donner la matrice de confusion et indiquer les languages les plus difficile à distinguer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "945     3\n4245    3\n3690    1\n3716    3\n3578    0\nName: lang, dtype: int64"
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "dfDE = pd.DataFrame(FeaDE)\n",
    "dfDE[\"lang\"]=0\n",
    "\n",
    "dfFR = pd.DataFrame(FeaFR)\n",
    "dfFR[\"lang\"]=1\n",
    "\n",
    "dfEN = pd.DataFrame(FeaEN)\n",
    "dfEN[\"lang\"]=2\n",
    "\n",
    "dfES = pd.DataFrame(FeaES)\n",
    "dfES[\"lang\"]=3\n",
    "\n",
    "df = pd.concat([dfDE, dfFR, dfEN, dfES])\n",
    "y = df.pop(\"lang\")\n",
    "X = df\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "y_train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "3 vs all \nAccuracy on test : 1.0\n1 vs all \nAccuracy on test : 1.0\n0 vs all \nAccuracy on test : 0.9997054491899853\n2 vs all \nAccuracy on test : 1.0\n"
    }
   ],
   "source": [
    "models = {}\n",
    "for value in y_train.unique():\n",
    "    print(f\"{value} vs all \")\n",
    "    y_train_1va = y_train.copy()\n",
    "    y_train_1va[y_train_1va==value]=-1\n",
    "    y_train_1va[y_train_1va>=0]=1\n",
    "    y_train_1va[y_train_1va==-1]=0\n",
    "\n",
    "    y_test_1va = y_test.copy()\n",
    "    y_test_1va[y_test_1va==value]=-1\n",
    "    y_test_1va[y_test_1va>=0]=1\n",
    "    y_test_1va[y_test_1va==-1]=0\n",
    "    logreg = LogisticRegression(solver=\"liblinear\", penalty=logreg_cv.best_params_[\"penalty\"],                 C=logreg_cv.best_params_['C'])\n",
    "\n",
    "    logreg.fit(X_train, y_train_1va)\n",
    "    y_pred = logreg.predict(X_test)\n",
    "    print(f\"Accuracy on test : {accuracy_score(y_test_1va, y_pred)}\")\n",
    "    models[value]=logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We initialize an array of zeros, which will contain the maximum probabilities seen for every test input winthin each model\n",
    "previousMaxProbs = np.zeros(y_test.shape[0])\n",
    "# We initialize an array of zeros which will contain the predicted class\n",
    "predictions = np.zeros(y_test.shape[0])\n",
    "\n",
    "# We iterate over the previously built models\n",
    "for value, model in models.items():\n",
    "    # We predict the probability for each input to belong to the class (vs the rest)\n",
    "    currentProbs = model.predict_proba(X_test)\n",
    "    # We update the max probabilities we've seen in past\n",
    "    previousMaxProbs = np.maximum(previousMaxProbs, currentProbs[:, 0])\n",
    "    # If for each input, the probabiliy to belong to this class was over the previous probability, we update the prediction\n",
    "    predictions[np.equal(currentProbs[:, 0],previousMaxProbs)]=value\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L'accuracy de 100% rend cette approche très performante. Etudions la matrice de confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([[844,   0,   0,   0],\n       [  0, 875,   0,   0],\n       [  0,   0, 833,   0],\n       [  0,   0,   0, 843]], dtype=int64)"
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cette approche permet une classification parfaite des données ! C'est une grand réussite.\n",
    "\n",
    "N'ayant plus de progès à réaliser, je me contente de laisser ce classifieur utilisant la régression linéaire avec une régularisation L1 et un C optimisé comme classifieur.\n",
    "\n",
    "Je suis cependant curieux de savoir si l'approche \"one vs one\" permet d'aboutir aux mêmes résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "On commence par définir les couples de données qui vont être classifiées ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[1, 3], [0, 3], [2, 3], [0, 1], [1, 2], [0, 2]]\n"
    }
   ],
   "source": [
    "array = []\n",
    "for value in y_train.unique():\n",
    "    for value2 in y_train.unique():\n",
    "        if(value != value2 and not array.__contains__(sorted([value, value2]))):\n",
    "\n",
    "            array.append(sorted([value, value2]))\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Puis on va sur Wikipedia parce qu'on ne sait pas comment ça marche..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2].sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2].sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}